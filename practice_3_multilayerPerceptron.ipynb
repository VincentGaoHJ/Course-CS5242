{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practice_3_multilayerPerceptron.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VincentGaoHJ/Course-CS5242/blob/master/practice_3_multilayerPerceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t--wDFP1sEdg"
      },
      "source": [
        "## Practice 3: Multilayer Networks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW7c6fznGFyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee42fc05-022a-4532-dd2d-5b3079f73c41"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading extenrnal modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gYBM8VgFgwp"
      },
      "source": [
        "**Pytorch has a wide variety of datasets that one can leverage. In this tutorial, we will\n",
        "use one such dataset and build a multilayer neural network to fit the data. We will \n",
        "use PyTorch optimizers to perform the optimization process.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oH7W65sq_Z9"
      },
      "source": [
        "Q1. Use the following snippet to get a dataloader from the already available Mnist dataset from \n",
        "PyTorch. Mnist dataset has 60,000 digit images with its corresponding labels.\n",
        "This snippet will download the dataset if not downloaded already and transform each \n",
        "image before returning it. Dataloaders are iterable over a dataset. For further reading follow [pytorch dataloader](https://pytorch.org/docs/stable/data.html)\n",
        "\n",
        "```python\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "traintloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZA4o635FW8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d83530b-700d-42a8-95ab-392bea9d23f5"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) \n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Code for the test dataset and data loader\n",
        "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Use the help command to see the dictionary elements, attributes and method of any object\n",
        "help(trainloader)"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on DataLoader in module torch.utils.data.dataloader object:\n",
            "\n",
            "class DataLoader(typing.Generic)\n",
            " |  Data loader. Combines a dataset and a sampler, and provides an iterable over\n",
            " |  the given dataset.\n",
            " |  \n",
            " |  The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
            " |  iterable-style datasets with single- or multi-process loading, customizing\n",
            " |  loading order and optional automatic batching (collation) and memory pinning.\n",
            " |  \n",
            " |  See :py:mod:`torch.utils.data` documentation page for more details.\n",
            " |  \n",
            " |  Arguments:\n",
            " |      dataset (Dataset): dataset from which to load the data.\n",
            " |      batch_size (int, optional): how many samples per batch to load\n",
            " |          (default: ``1``).\n",
            " |      shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
            " |          at every epoch (default: ``False``).\n",
            " |      sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
            " |          samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
            " |          implemented. If specified, :attr:`shuffle` must not be specified.\n",
            " |      batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
            " |          returns a batch of indices at a time. Mutually exclusive with\n",
            " |          :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
            " |          and :attr:`drop_last`.\n",
            " |      num_workers (int, optional): how many subprocesses to use for data\n",
            " |          loading. ``0`` means that the data will be loaded in the main process.\n",
            " |          (default: ``0``)\n",
            " |      collate_fn (callable, optional): merges a list of samples to form a\n",
            " |          mini-batch of Tensor(s).  Used when using batched loading from a\n",
            " |          map-style dataset.\n",
            " |      pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
            " |          into CUDA pinned memory before returning them.  If your data elements\n",
            " |          are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
            " |          see the example below.\n",
            " |      drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
            " |          if the dataset size is not divisible by the batch size. If ``False`` and\n",
            " |          the size of dataset is not divisible by the batch size, then the last batch\n",
            " |          will be smaller. (default: ``False``)\n",
            " |      timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
            " |          from workers. Should always be non-negative. (default: ``0``)\n",
            " |      worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
            " |          worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
            " |          input, after seeding and before data loading. (default: ``None``)\n",
            " |      prefetch_factor (int, optional, keyword-only arg): Number of sample loaded\n",
            " |          in advance by each worker. ``2`` means there will be a total of\n",
            " |          2 * num_workers samples prefetched across all workers. (default: ``2``)\n",
            " |      persistent_workers (bool, optional): If ``True``, the data loader will not shutdown\n",
            " |          the worker processes after a dataset has been consumed once. This allows to \n",
            " |          maintain the workers `Dataset` instances alive. (default: ``False``)\n",
            " |  \n",
            " |  \n",
            " |  .. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
            " |               cannot be an unpicklable object, e.g., a lambda function. See\n",
            " |               :ref:`multiprocessing-best-practices` on more details related\n",
            " |               to multiprocessing in PyTorch.\n",
            " |  \n",
            " |  .. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
            " |               When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
            " |               it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
            " |               rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
            " |               configurations. This represents the best guess PyTorch can make because PyTorch\n",
            " |               trusts user :attr:`dataset` code in correctly handling multi-process\n",
            " |               loading to avoid duplicate data.\n",
            " |  \n",
            " |               However, if sharding results in multiple workers having incomplete last batches,\n",
            " |               this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
            " |               be broken into multiple ones and (2) more than one batch worth of samples can be\n",
            " |               dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
            " |               cases in general.\n",
            " |  \n",
            " |               See `Dataset Types`_ for more details on these two types of datasets and how\n",
            " |               :class:`~torch.utils.data.IterableDataset` interacts with\n",
            " |               `Multi-process data loading`_.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      DataLoader\n",
            " |      typing.Generic\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, dataset:torch.utils.data.dataset.Dataset, batch_size:Union[int, NoneType]=1, shuffle:bool=False, sampler:Union[torch.utils.data.sampler.Sampler[int], NoneType]=None, batch_sampler:Union[torch.utils.data.sampler.Sampler[Sequence[int]], NoneType]=None, num_workers:int=0, collate_fn:Callable[[List[~T]], Any]=None, pin_memory:bool=False, drop_last:bool=False, timeout:float=0, worker_init_fn:Callable[[int], NoneType]=None, multiprocessing_context=None, generator=None, *, prefetch_factor:int=2, persistent_workers:bool=False)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __iter__(self) -> '_BaseDataLoaderIter'\n",
            " |      # We quote '_BaseDataLoaderIter' since it isn't defined yet and the definition can't be moved up\n",
            " |      # since '_BaseDataLoaderIter' references 'DataLoader'.\n",
            " |  \n",
            " |  __len__(self) -> int\n",
            " |  \n",
            " |  __setattr__(self, attr, val)\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  multiprocessing_context\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  __annotations__ = {'_iterator': typing.Union[_ForwardRef('_BaseDataLoa...\n",
            " |  \n",
            " |  __args__ = None\n",
            " |  \n",
            " |  __extra__ = None\n",
            " |  \n",
            " |  __next_in_mro__ = <class 'object'>\n",
            " |      The most base type\n",
            " |  \n",
            " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
            " |  \n",
            " |  __origin__ = None\n",
            " |  \n",
            " |  __parameters__ = (+T_co,)\n",
            " |  \n",
            " |  __tree_hash__ = -9223372036848650691\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from typing.Generic:\n",
            " |  \n",
            " |  __new__(cls, *args, **kwds)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmvPROOEHwGY"
      },
      "source": [
        "Q2. Wrap the dataloader with python “iter” class which will give you an iterable object from which you can get new images and labels using .next(). After applying iter, iterate through a few samples, check the shape, the datatype of the image, label tuple that you are getting. Plot some of the images. Also, print the flattened shape of each image. (hint: use .flatten())\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi27wJQitA_m",
        "outputId": "0e84ccb4-e8c5-46b6-c48e-2d3cff9dd736"
      },
      "source": [
        "print('Dataset: ', trainloader.dataset)\r\n",
        "print('samples per batch: ', trainloader.batch_size)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset:  Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: /root/.pytorch/MNIST_data/\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=(0.5,), std=(0.5,))\n",
            "           )\n",
            "samples per batch:  64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CroqW1JWr8or",
        "outputId": "e4690b29-d0e3-488f-da27-8a562604090b"
      },
      "source": [
        "# Here we are doing some initial analysis of what .next of dataloader returns\r\n",
        "train_iter = iter(trainloader)\r\n",
        "batches, labels = train_iter.next()\r\n",
        "\r\n",
        "# check the shape\r\n",
        "print('Tensor shape: ', batches.shape)\r\n",
        "# the datatype of the image\r\n",
        "print('Datatype of image: ', batches.type)\r\n",
        "# label tuple that you are getting\r\n",
        "print('Label tuple: ', labels)\r\n",
        "# Also, print the flattened shape of each image. (hint: use .flatten())\r\n",
        "print('Flattened shape of each image: ', batches[0].flatten().shape)"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor shape:  torch.Size([64, 1, 28, 28])\n",
            "Datatype of image:  <built-in method type of Tensor object at 0x7efd1b512288>\n",
            "Label tuple:  tensor([9, 5, 0, 7, 4, 9, 0, 8, 1, 8, 8, 2, 9, 0, 6, 1, 4, 7, 2, 2, 1, 8, 7, 6,\n",
            "        9, 4, 1, 2, 8, 3, 5, 0, 9, 1, 4, 0, 4, 2, 0, 0, 3, 5, 9, 4, 0, 8, 0, 4,\n",
            "        8, 4, 4, 6, 4, 4, 7, 1, 0, 4, 0, 7, 9, 3, 7, 0])\n",
            "Flattened shape of each image:  torch.Size([784])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "KNZUQobdutI5",
        "outputId": "13bea43a-00a6-4637-f8dc-57ef4aa95a61"
      },
      "source": [
        "np.random.seed(0)     # We need to set the seed to help in reproducibility of data\r\n",
        "\r\n",
        "length_of_dict_class = 10 # This indicates we have 10 labels corresponding to Mnist dataset\r\n",
        "dict_class = {0:\"zero\", 1:\"one\", 2: \"two\", 3: \"three\", 4:\"four\", 5:\"five\", 6:\"six\", 7:\"seven\", 8:\"eight\", 9:\"nine\"}\r\n",
        "\r\n",
        "# Since we notice the dimension of image is 1 * 28 * 28, this means image is gray scale, \r\n",
        "# it has only one channel, we should squeeze the extra dimension\r\n",
        "\r\n",
        "\r\n",
        "def visualize_images(examples_per_class):\r\n",
        "    \r\n",
        "    for cls in range(length_of_dict_class):\r\n",
        "        idxs = np.where((labels == cls))[0] # Find index of the specific class\r\n",
        "        # If in this batch we do not get the required number of samples we want to show, skip\r\n",
        "        if len(idxs) >= examples_per_class:\r\n",
        "            idxs = np.random.choice(idxs, examples_per_class, replace=False) \r\n",
        "        \r\n",
        "        # Plot here\r\n",
        "        for enu_idx, img_idx in enumerate(idxs):\r\n",
        "            plt_index = enu_idx * length_of_dict_class + cls + 1\r\n",
        "            plt.subplot(examples_per_class, length_of_dict_class, plt_index)\r\n",
        "            plt.imshow(torch.squeeze(batches[img_idx]))\r\n",
        "            plt.axis('off')\r\n",
        "            if enu_idx == 0:\r\n",
        "                plt.title(dict_class[cls])\r\n",
        "    \r\n",
        "visualize_images(5)"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 44 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkEIv5JZH02X"
      },
      "source": [
        "\n",
        "Q3. Build a multilayer neural network with 3 fully connected layers of shape (784, 128), (128, 64) and then (64, 10). Use ReLu activation in between each layer, and at the end of the output layer. \n",
        "The class should have a forward function that you will manually create. The forward will take image batch as an input (image_batch * flattened_image_784_size) and return (image_batch x 10) size vectors.\n",
        "\n",
        "**Hint**: Use torch.nn.Linear for linear layers. Follow this pytorch module to understand how to inherit the [pytorch module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIkPZ0nrCBSN",
        "outputId": "88eb5196-ae86-4fba-8833-c6929a8400ae"
      },
      "source": [
        "# Declare device which you will be working with\r\n",
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "device"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCBKM8B5H3s2"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Multilayer neural network with 3 fully connected layers of shape \n",
        "    (784, 128) -> (128, 64) -> (64, 10)\n",
        "    \"\"\"\n",
        "\n",
        "    # Use ReLu activation in between each layer, and at the end of the output layer. \n",
        "    # The class should have a forward function that you will manually create. \n",
        "    # The forward will take image batch as an input (image_batch * flattened_image_784_size) and return (image_batch x 10) size vectors.\n",
        "    def __init__(self):\n",
        "        # Creates an instance of the base nn.Module class\n",
        "        super(Model, self).__init__() # Inheritance of the base class nn.Module\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(784, 128) # 28 x 28 input pixels connecting to the first hidden layer (128 node)\n",
        "        self.fc2 = nn.Linear(128, 64) # 128 to 64 hidden layer\n",
        "        self.fc3 = nn.Linear(64, 10) # last hidden layer to the output layer (with 10 nodes).\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# network = Model().to(device)\n",
        "# print(network)"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhhDVNdMH5GM"
      },
      "source": [
        "Q4. Create a cross-entropy loss. You can create it yourself (just like the last tutorial) or create it by using torch.nn.CrossEntropyLoss (pytorch cross-entropy loss uses logits, not softmax probabilities, so we had not applied softmax at the last layer). Initialize an SGD optimizer from torch.optim. \n",
        "\n",
        "**PyTorch Loss-Input Confusion (Cheatsheet)**\n",
        "\n",
        "*   torch.nn.functional.binary_cross_entropy takes logistic sigmoid values as inputs\n",
        "*   torch.nn.functional.binary_cross_entropy_with_logits takes logits as inputs\n",
        "*   torch.nn.functional.cross_entropy takes logits as inputs (performs log_softmax internally)\n",
        "*   torch.nn.functional.nll_loss is like cross_entropy but takes log-probabilities (log-softmax) values as inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Wn9KGV9H8OM"
      },
      "source": [
        "# used 0.1 learning rate\n",
        "learning_rate = 0.1\n",
        "momentum = 0\n",
        "# create a stochastic gradient descent optimizer\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum) \n",
        "# create a loss function\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tuGd0H2H9GD"
      },
      "source": [
        "\n",
        "Q5. (1 pass of the data) Write a training loop which will \n",
        "1.\tGet data batches from dataloader, feed it into the network and get outputs.\n",
        "2.\tGet loss using outputs and ground truth labels.\n",
        "3.\tCall backward on the loss (remember to clear grads before calling gradients)\n",
        "4.\tCall optimizer.step to perform SGD update.\n",
        "5.\tPrint the accuracy, loss and amount of data used till now.\n",
        "\n",
        "\n",
        "**Note**: We have used only training split in this week tutorial for sake of simplicity of the tutorial. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89yBKAZyf98b",
        "outputId": "b9c5a5bc-6073-4905-9754-e607f75d8f2c"
      },
      "source": [
        "def cal_acc(outputs, labels):\r\n",
        "    # probs: probability that each image is labeled as 1\r\n",
        "    # target: ground truth label\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    with torch.no_grad():\r\n",
        "        _, predicted = torch.max(outputs.data, 1)\r\n",
        "        total += labels.size(0)\r\n",
        "        correct += (predicted == labels).sum().item()\r\n",
        "    return 100 * correct / total\r\n",
        "\r\n",
        "def train_one_pass(p_network, criterion, p_optim, trainloader, epoch=1, log_interval=100):\r\n",
        "    p_network.train()\r\n",
        "    acc_one_pass, loss_one_pass = [], []\r\n",
        "    for idx, (images, labels) in enumerate(trainloader):\r\n",
        "        # Move tensors to the configured device\r\n",
        "        images = images.reshape(-1, 28*28).to(device) # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\r\n",
        "        labels = labels.to(device)\r\n",
        "        \r\n",
        "        # Forward pass\r\n",
        "        model_out = p_network(images)\r\n",
        "        loss = criterion(model_out, labels) # Get loss using outputs and ground truth labels.\r\n",
        "        \r\n",
        "        # Backprpagation and optimization\r\n",
        "        p_optim.zero_grad() # remember to clear grads before calling gradients\r\n",
        "        loss.backward() # Call backward on the loss ()\r\n",
        "        p_optim.step() # Call optimizer.step to perform SGD update.\r\n",
        "        \r\n",
        "        acc = cal_acc(model_out, labels) # Print the accuracy, loss and amount of data used till now.\r\n",
        "\r\n",
        "        # Print Information\r\n",
        "        if (idx + 1) % log_interval == 0:\r\n",
        "            print('Iter: [{}/{} ({:.2f}%)] \\t Train Loss: {:.4f} \\t Train Accuracy: {:.2f}%'.format(\r\n",
        "                (idx+1) * len(images), len(trainset), \r\n",
        "                100.*(idx+1) * len(images)/len(trainset), loss, acc))\r\n",
        "        acc_one_pass.append(acc)\r\n",
        "        loss_one_pass.append(loss)\r\n",
        "    \r\n",
        "    avg_acc = sum(acc_one_pass) / len(acc_one_pass)\r\n",
        "    avg_loss = sum(loss_one_pass) / len(loss_one_pass)\r\n",
        "    print('Train on Epoch: {} \\t Loss: {:.4f} \\t Accuracy: {:.2f}%'.format(\r\n",
        "        epoch, avg_loss, avg_acc).center(76, '='))\r\n",
        "\r\n",
        "\r\n",
        "def test_one_pass(p_network, testloader, epoch=1):\r\n",
        "    p_network.eval()\r\n",
        "    loss_lst, acc_lst = [], []\r\n",
        "    with torch.no_grad():\r\n",
        "        for images, labels in testloader:\r\n",
        "            images = images.reshape(-1, 28*28).to(device) # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\r\n",
        "            labels = labels.to(device)\r\n",
        "            model_out = p_network(images)\r\n",
        "            loss = criterion(model_out, labels)\r\n",
        "            acc = cal_acc(model_out, labels)\r\n",
        "            loss_lst.append(loss)\r\n",
        "            acc_lst.append(acc)\r\n",
        "    loss = sum(loss_lst) / len(loss_lst)\r\n",
        "    acc = sum(acc_lst) / len(acc_lst)\r\n",
        "    print('Test on Epoch: {} \\t Loss: {:.4f} \\t Accuracy: {:.2f}%'.format(\r\n",
        "        epoch, loss, acc).center(76, '='))\r\n",
        "\r\n",
        "network = Model().to(device)\r\n",
        "\r\n",
        "# Loss and optimizer\r\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr=0.01, momentum=momentum) \r\n",
        "\r\n",
        "# Train the model\r\n",
        "train_one_pass(network, criterion, optimizer, trainloader)\r\n",
        "\r\n",
        "# Test the model\r\n",
        "test_one_pass(network, testloader)"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: [6400/60000 (10.67%)] \t Train Loss: 2.1583 \t Train Accuracy: 40.62%\n",
            "Iter: [12800/60000 (21.33%)] \t Train Loss: 1.6513 \t Train Accuracy: 60.94%\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 1.1374 \t Train Accuracy: 71.88%\n",
            "Iter: [25600/60000 (42.67%)] \t Train Loss: 0.9190 \t Train Accuracy: 75.00%\n",
            "Iter: [32000/60000 (53.33%)] \t Train Loss: 0.6841 \t Train Accuracy: 82.81%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.6448 \t Train Accuracy: 87.50%\n",
            "Iter: [44800/60000 (74.67%)] \t Train Loss: 0.5599 \t Train Accuracy: 81.25%\n",
            "Iter: [51200/60000 (85.33%)] \t Train Loss: 0.4814 \t Train Accuracy: 85.94%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.4426 \t Train Accuracy: 81.25%\n",
            "============Train on Epoch: 1 \t Loss: 1.0552 \t Accuracy: 72.00%=============\n",
            "=============Test on Epoch: 1 \t Loss: 0.4545 \t Accuracy: 86.37%=============\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBLLqaCWH_sY"
      },
      "source": [
        "\n",
        "Q6. (Multiple Passes of Data) Write a loop to perform the training pass for 50 training iterations (50 epochs, this is outer for loop of data loader). Print and save accuracy and loss after each epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy0JoCSziO5t",
        "outputId": "21ced675-e9b7-4955-8576-0c3c8e1e95b2"
      },
      "source": [
        "def multiple_pass(model, criterion, optimizer, train_loader, testloader, num_epochs=10, log_interval=300):\r\n",
        "    # Train the model\r\n",
        "    total_step = len(train_loader)\r\n",
        "    for epoch in range(1, num_epochs + 1):\r\n",
        "        train_one_pass(model, criterion, optimizer, train_loader, \r\n",
        "                       epoch=epoch, log_interval=log_interval)\r\n",
        "        test_one_pass(model, testloader, \r\n",
        "                      epoch=epoch)\r\n",
        "\r\n",
        "network = Model().to(device)\r\n",
        "\r\n",
        "# Loss and optimizer\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr=0.01, momentum=momentum) \r\n",
        "\r\n",
        "# Train and Test\r\n",
        "multiple_pass(network, criterion, optimizer, trainloader, testloader,\r\n",
        "              num_epochs=10)"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 1.1691 \t Train Accuracy: 78.12%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.7226 \t Train Accuracy: 79.69%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.3608 \t Train Accuracy: 93.75%\n",
            "============Train on Epoch: 1 \t Loss: 1.0104 \t Accuracy: 71.39%=============\n",
            "=============Test on Epoch: 1 \t Loss: 0.4279 \t Accuracy: 88.19%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.2429 \t Train Accuracy: 95.31%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.2030 \t Train Accuracy: 95.31%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.4817 \t Train Accuracy: 84.38%\n",
            "============Train on Epoch: 2 \t Loss: 0.3835 \t Accuracy: 89.08%=============\n",
            "=============Test on Epoch: 2 \t Loss: 0.3331 \t Accuracy: 90.26%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.2352 \t Train Accuracy: 93.75%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.2316 \t Train Accuracy: 90.62%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.1975 \t Train Accuracy: 95.31%\n",
            "============Train on Epoch: 3 \t Loss: 0.3235 \t Accuracy: 90.58%=============\n",
            "=============Test on Epoch: 3 \t Loss: 0.2924 \t Accuracy: 91.32%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.1708 \t Train Accuracy: 96.88%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.3865 \t Train Accuracy: 89.06%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.1664 \t Train Accuracy: 93.75%\n",
            "============Train on Epoch: 4 \t Loss: 0.2917 \t Accuracy: 91.41%=============\n",
            "=============Test on Epoch: 4 \t Loss: 0.2853 \t Accuracy: 91.52%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.3688 \t Train Accuracy: 85.94%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.2553 \t Train Accuracy: 93.75%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.1922 \t Train Accuracy: 95.31%\n",
            "============Train on Epoch: 5 \t Loss: 0.2676 \t Accuracy: 92.10%=============\n",
            "=============Test on Epoch: 5 \t Loss: 0.2554 \t Accuracy: 92.41%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.2113 \t Train Accuracy: 92.19%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.1692 \t Train Accuracy: 93.75%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.2259 \t Train Accuracy: 95.31%\n",
            "============Train on Epoch: 6 \t Loss: 0.2459 \t Accuracy: 92.85%=============\n",
            "=============Test on Epoch: 6 \t Loss: 0.2266 \t Accuracy: 93.36%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.2704 \t Train Accuracy: 90.62%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.2356 \t Train Accuracy: 95.31%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.1793 \t Train Accuracy: 93.75%\n",
            "============Train on Epoch: 7 \t Loss: 0.2265 \t Accuracy: 93.42%=============\n",
            "=============Test on Epoch: 7 \t Loss: 0.2198 \t Accuracy: 93.57%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.2253 \t Train Accuracy: 92.19%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.2269 \t Train Accuracy: 93.75%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.2910 \t Train Accuracy: 93.75%\n",
            "============Train on Epoch: 8 \t Loss: 0.2087 \t Accuracy: 93.83%=============\n",
            "=============Test on Epoch: 8 \t Loss: 0.2004 \t Accuracy: 94.12%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.1804 \t Train Accuracy: 90.62%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.1907 \t Train Accuracy: 95.31%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.1875 \t Train Accuracy: 98.44%\n",
            "============Train on Epoch: 9 \t Loss: 0.1929 \t Accuracy: 94.37%=============\n",
            "=============Test on Epoch: 9 \t Loss: 0.1861 \t Accuracy: 94.41%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.2438 \t Train Accuracy: 93.75%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.1676 \t Train Accuracy: 95.31%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.3164 \t Train Accuracy: 92.19%\n",
            "============Train on Epoch: 10 \t Loss: 0.1792 \t Accuracy: 94.75%============\n",
            "============Test on Epoch: 10 \t Loss: 0.1738 \t Accuracy: 94.70%=============\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhIF4yegIBxb"
      },
      "source": [
        "\n",
        "Q7. Play around with the momentum of the SGD optimizer and then replace the SGD optimizer with Adam and RMSProp and analyze the loss trajectory (save the value of loss in an array after each update) of 50 training iterations for each of these types of optimizers. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pke4C-BpIDKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b0f2e44-cb52-4ad8-ecdd-61dc37e02249"
      },
      "source": [
        "network = Model().to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "adam_optim = torch.optim.Adam(network.parameters(), lr=0.01) \n",
        "\n",
        "# Train and Test\n",
        "multiple_pass(network, criterion, adam_optim, trainloader, testloader,\n",
        "              num_epochs=10)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.1747 \t Train Accuracy: 92.19%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.2565 \t Train Accuracy: 92.19%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.2517 \t Train Accuracy: 92.19%\n",
            "============Train on Epoch: 1 \t Loss: 0.3899 \t Accuracy: 88.04%=============\n",
            "=============Test on Epoch: 1 \t Loss: 0.2676 \t Accuracy: 92.34%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.1213 \t Train Accuracy: 95.31%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.1626 \t Train Accuracy: 93.75%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.2421 \t Train Accuracy: 92.19%\n",
            "============Train on Epoch: 2 \t Loss: 0.2500 \t Accuracy: 92.62%=============\n",
            "=============Test on Epoch: 2 \t Loss: 0.2099 \t Accuracy: 93.80%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.3708 \t Train Accuracy: 92.19%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.4265 \t Train Accuracy: 87.50%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.0350 \t Train Accuracy: 100.00%\n",
            "============Train on Epoch: 3 \t Loss: 0.2356 \t Accuracy: 93.32%=============\n",
            "=============Test on Epoch: 3 \t Loss: 0.2451 \t Accuracy: 93.01%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.3152 \t Train Accuracy: 89.06%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.1811 \t Train Accuracy: 96.88%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.1233 \t Train Accuracy: 93.75%\n",
            "============Train on Epoch: 4 \t Loss: 0.2252 \t Accuracy: 93.65%=============\n",
            "=============Test on Epoch: 4 \t Loss: 0.2181 \t Accuracy: 93.90%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.1230 \t Train Accuracy: 93.75%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.1097 \t Train Accuracy: 96.88%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.1838 \t Train Accuracy: 92.19%\n",
            "============Train on Epoch: 5 \t Loss: 0.2052 \t Accuracy: 94.25%=============\n",
            "=============Test on Epoch: 5 \t Loss: 0.3009 \t Accuracy: 92.20%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.1215 \t Train Accuracy: 96.88%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.0803 \t Train Accuracy: 98.44%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.1595 \t Train Accuracy: 95.31%\n",
            "============Train on Epoch: 6 \t Loss: 0.2029 \t Accuracy: 94.39%=============\n",
            "=============Test on Epoch: 6 \t Loss: 0.2128 \t Accuracy: 94.43%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.2966 \t Train Accuracy: 95.31%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.1493 \t Train Accuracy: 96.88%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.1593 \t Train Accuracy: 93.75%\n",
            "============Train on Epoch: 7 \t Loss: 0.2007 \t Accuracy: 94.62%=============\n",
            "=============Test on Epoch: 7 \t Loss: 0.1752 \t Accuracy: 95.19%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.2948 \t Train Accuracy: 93.75%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.1303 \t Train Accuracy: 95.31%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.2188 \t Train Accuracy: 92.19%\n",
            "============Train on Epoch: 8 \t Loss: 0.1912 \t Accuracy: 94.86%=============\n",
            "=============Test on Epoch: 8 \t Loss: 0.2203 \t Accuracy: 94.70%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.0614 \t Train Accuracy: 98.44%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.2051 \t Train Accuracy: 92.19%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.0677 \t Train Accuracy: 96.88%\n",
            "============Train on Epoch: 9 \t Loss: 0.1879 \t Accuracy: 94.96%=============\n",
            "=============Test on Epoch: 9 \t Loss: 0.2130 \t Accuracy: 94.36%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.1538 \t Train Accuracy: 95.31%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.1069 \t Train Accuracy: 98.44%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.0553 \t Train Accuracy: 98.44%\n",
            "============Train on Epoch: 10 \t Loss: 0.1849 \t Accuracy: 95.16%============\n",
            "============Test on Epoch: 10 \t Loss: 0.2010 \t Accuracy: 95.16%=============\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGKzJ3b3ID6h"
      },
      "source": [
        "\n",
        "Q8. Play around with the initialization function like uniform and gaussian learnt in the lecture slides.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xoeVDWqZolb",
        "outputId": "f8670ad5-53c7-4503-a610-e646d1e23c49"
      },
      "source": [
        "def init_weights_uniform(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        m.weight.data.uniform_(0.0, 1.0) # uniform distribution(bias=0)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "# Initialize Weight\n",
        "network_initialized = Model().to(device)\n",
        "network_initialized.apply(init_weights_uniform)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "adam_optim = torch.optim.Adam(network_initialized.parameters(), lr=0.01) \n",
        "\n",
        "# Train and Test\n",
        "print(\"Now running the uniformed initialized model with Adam Optimizer\")\n",
        "multiple_pass(network_initialized, criterion, adam_optim, trainloader, testloader,\n",
        "              num_epochs=10)"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now running the uniformed initialized model with Adam Optimizer\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 2.2964 \t Train Accuracy: 7.81%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 2.2987 \t Train Accuracy: 12.50%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 2.3042 \t Train Accuracy: 7.81%\n",
            "============Train on Epoch: 1 \t Loss: 2.3021 \t Accuracy: 10.97%=============\n",
            "=============Test on Epoch: 1 \t Loss: 2.3014 \t Accuracy: 11.36%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 2.3021 \t Train Accuracy: 10.94%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 2.2904 \t Train Accuracy: 17.19%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 2.2963 \t Train Accuracy: 18.75%\n",
            "============Train on Epoch: 2 \t Loss: 2.3021 \t Accuracy: 10.92%=============\n",
            "=============Test on Epoch: 2 \t Loss: 2.3020 \t Accuracy: 11.45%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 2.3152 \t Train Accuracy: 9.38%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 2.3031 \t Train Accuracy: 10.94%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 2.3098 \t Train Accuracy: 7.81%\n",
            "============Train on Epoch: 3 \t Loss: 2.3021 \t Accuracy: 11.04%=============\n",
            "=============Test on Epoch: 3 \t Loss: 2.3014 \t Accuracy: 11.36%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 2.2888 \t Train Accuracy: 14.06%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 2.2856 \t Train Accuracy: 14.06%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 2.3103 \t Train Accuracy: 10.94%\n",
            "============Train on Epoch: 4 \t Loss: 2.3022 \t Accuracy: 11.05%=============\n",
            "=============Test on Epoch: 4 \t Loss: 2.3011 \t Accuracy: 11.47%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 2.3122 \t Train Accuracy: 3.12%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 2.3028 \t Train Accuracy: 9.38%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 2.2924 \t Train Accuracy: 15.62%\n",
            "============Train on Epoch: 5 \t Loss: 2.3021 \t Accuracy: 11.00%=============\n",
            "=============Test on Epoch: 5 \t Loss: 2.3017 \t Accuracy: 11.30%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 2.2835 \t Train Accuracy: 21.88%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 2.3303 \t Train Accuracy: 3.12%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 2.3008 \t Train Accuracy: 7.81%\n",
            "============Train on Epoch: 6 \t Loss: 2.3021 \t Accuracy: 10.95%=============\n",
            "=============Test on Epoch: 6 \t Loss: 2.3032 \t Accuracy: 10.08%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 2.3045 \t Train Accuracy: 4.69%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 2.2912 \t Train Accuracy: 15.62%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 2.2981 \t Train Accuracy: 12.50%\n",
            "============Train on Epoch: 7 \t Loss: 2.3022 \t Accuracy: 11.03%=============\n",
            "=============Test on Epoch: 7 \t Loss: 2.3020 \t Accuracy: 11.30%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 2.3095 \t Train Accuracy: 10.94%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 2.3161 \t Train Accuracy: 6.25%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 2.3005 \t Train Accuracy: 17.19%\n",
            "============Train on Epoch: 8 \t Loss: 2.3022 \t Accuracy: 11.05%=============\n",
            "=============Test on Epoch: 8 \t Loss: 2.3013 \t Accuracy: 11.39%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 2.3033 \t Train Accuracy: 10.94%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 2.2939 \t Train Accuracy: 15.62%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 2.2934 \t Train Accuracy: 10.94%\n",
            "============Train on Epoch: 9 \t Loss: 2.3022 \t Accuracy: 10.99%=============\n",
            "=============Test on Epoch: 9 \t Loss: 2.3020 \t Accuracy: 11.30%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 2.3082 \t Train Accuracy: 6.25%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 2.2999 \t Train Accuracy: 6.25%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 2.3029 \t Train Accuracy: 4.69%\n",
            "============Train on Epoch: 10 \t Loss: 2.3022 \t Accuracy: 11.11%============\n",
            "============Test on Epoch: 10 \t Loss: 2.3017 \t Accuracy: 11.33%=============\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntNK9TIooIy7",
        "outputId": "3009acc1-333f-4f7c-e8ba-7357c7930ba6"
      },
      "source": [
        "def weights_init_normal(m):\r\n",
        "        classname = m.__class__.__name__\r\n",
        "        if classname.find('Linear') != -1:\r\n",
        "            y = m.in_features\r\n",
        "            m.weight.data.normal_(0.0,1/np.sqrt(y))\r\n",
        "            m.bias.data.fill_(0)\r\n",
        "\r\n",
        "# Initialize Weight\r\n",
        "network_initialized = Model().to(device)\r\n",
        "network_initialized.apply(weights_init_normal)\r\n",
        "\r\n",
        "# Loss and optimizer\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "adam_optim = torch.optim.Adam(network_initialized.parameters(), lr=0.01) \r\n",
        "\r\n",
        "# Train and Test\r\n",
        "print(\"Now running the normal distribution initialized model with Adam Optimizer\")\r\n",
        "multiple_pass(network_initialized, criterion, adam_optim, trainloader, testloader,\r\n",
        "              num_epochs=10)"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now running the normal distribution initialized model with Adam Optimizer\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.6961 \t Train Accuracy: 89.06%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.4315 \t Train Accuracy: 82.81%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.2836 \t Train Accuracy: 90.62%\n",
            "============Train on Epoch: 1 \t Loss: 0.4067 \t Accuracy: 87.76%=============\n",
            "=============Test on Epoch: 1 \t Loss: 0.2372 \t Accuracy: 92.87%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.2362 \t Train Accuracy: 93.75%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.1793 \t Train Accuracy: 95.31%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.2599 \t Train Accuracy: 92.19%\n",
            "============Train on Epoch: 2 \t Loss: 0.2424 \t Accuracy: 92.77%=============\n",
            "=============Test on Epoch: 2 \t Loss: 0.2320 \t Accuracy: 93.07%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.1367 \t Train Accuracy: 96.88%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.1626 \t Train Accuracy: 93.75%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.2533 \t Train Accuracy: 92.19%\n",
            "============Train on Epoch: 3 \t Loss: 0.2253 \t Accuracy: 93.51%=============\n",
            "=============Test on Epoch: 3 \t Loss: 0.2372 \t Accuracy: 93.04%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.0903 \t Train Accuracy: 96.88%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.2571 \t Train Accuracy: 92.19%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.1338 \t Train Accuracy: 96.88%\n",
            "============Train on Epoch: 4 \t Loss: 0.2192 \t Accuracy: 93.81%=============\n",
            "=============Test on Epoch: 4 \t Loss: 0.2567 \t Accuracy: 92.96%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.1887 \t Train Accuracy: 95.31%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.3286 \t Train Accuracy: 92.19%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.3443 \t Train Accuracy: 90.62%\n",
            "============Train on Epoch: 5 \t Loss: 0.2126 \t Accuracy: 94.14%=============\n",
            "=============Test on Epoch: 5 \t Loss: 0.2059 \t Accuracy: 94.36%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.1531 \t Train Accuracy: 92.19%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.1648 \t Train Accuracy: 92.19%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.2104 \t Train Accuracy: 96.88%\n",
            "============Train on Epoch: 6 \t Loss: 0.2135 \t Accuracy: 94.03%=============\n",
            "=============Test on Epoch: 6 \t Loss: 0.2102 \t Accuracy: 94.44%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.0516 \t Train Accuracy: 98.44%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.3304 \t Train Accuracy: 90.62%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.3508 \t Train Accuracy: 90.62%\n",
            "============Train on Epoch: 7 \t Loss: 0.2009 \t Accuracy: 94.59%=============\n",
            "=============Test on Epoch: 7 \t Loss: 0.2074 \t Accuracy: 94.61%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.4351 \t Train Accuracy: 93.75%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.1330 \t Train Accuracy: 96.88%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.1117 \t Train Accuracy: 95.31%\n",
            "============Train on Epoch: 8 \t Loss: 0.2001 \t Accuracy: 94.55%=============\n",
            "=============Test on Epoch: 8 \t Loss: 0.1967 \t Accuracy: 94.69%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.2645 \t Train Accuracy: 95.31%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.1656 \t Train Accuracy: 95.31%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.2406 \t Train Accuracy: 96.88%\n",
            "============Train on Epoch: 9 \t Loss: 0.1940 \t Accuracy: 94.82%=============\n",
            "=============Test on Epoch: 9 \t Loss: 0.2475 \t Accuracy: 93.47%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.2195 \t Train Accuracy: 92.19%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.3055 \t Train Accuracy: 89.06%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.1667 \t Train Accuracy: 92.19%\n",
            "============Train on Epoch: 10 \t Loss: 0.2009 \t Accuracy: 94.70%============\n",
            "============Test on Epoch: 10 \t Loss: 0.2300 \t Accuracy: 93.91%=============\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSNwcQwcIGEG"
      },
      "source": [
        "\n",
        "Q9. Add batch normalization to each of the layers before ReLu activation function. Check how your results or iterations vary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1znPChEaWyob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6d608f0-fed0-41cd-eedf-77a2c67ef277"
      },
      "source": [
        "class Model_Normalization(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model_Normalization, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(num_features=128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(num_features=64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize Weight\n",
        "network_initialized = Model_Normalization().to(device)\n",
        "network_initialized.apply(weights_init_normal)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "adam_optim = torch.optim.Adam(network_initialized.parameters(), lr=0.01) \n",
        "\n",
        "# Train and Test\n",
        "print(\"Now running the normal distribution initialized model with Adam Optimizer\")\n",
        "multiple_pass(network_initialized, criterion, adam_optim, trainloader, testloader,\n",
        "              num_epochs=10)"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now running the normal distribution initialized model with Adam Optimizer\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.0947 \t Train Accuracy: 98.44%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.2248 \t Train Accuracy: 98.44%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.2623 \t Train Accuracy: 93.75%\n",
            "============Train on Epoch: 1 \t Loss: 0.2025 \t Accuracy: 93.72%=============\n",
            "=============Test on Epoch: 1 \t Loss: 0.1104 \t Accuracy: 96.37%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.0677 \t Train Accuracy: 96.88%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.1773 \t Train Accuracy: 96.88%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.1185 \t Train Accuracy: 95.31%\n",
            "============Train on Epoch: 2 \t Loss: 0.1032 \t Accuracy: 96.80%=============\n",
            "=============Test on Epoch: 2 \t Loss: 0.0914 \t Accuracy: 97.18%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.0449 \t Train Accuracy: 98.44%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.0558 \t Train Accuracy: 98.44%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.0578 \t Train Accuracy: 98.44%\n",
            "============Train on Epoch: 3 \t Loss: 0.0798 \t Accuracy: 97.48%=============\n",
            "=============Test on Epoch: 3 \t Loss: 0.0878 \t Accuracy: 97.25%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.0745 \t Train Accuracy: 98.44%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.1422 \t Train Accuracy: 95.31%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.1234 \t Train Accuracy: 96.88%\n",
            "============Train on Epoch: 4 \t Loss: 0.0675 \t Accuracy: 97.83%=============\n",
            "=============Test on Epoch: 4 \t Loss: 0.0865 \t Accuracy: 97.48%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.0354 \t Train Accuracy: 98.44%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.0214 \t Train Accuracy: 100.00%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.0955 \t Train Accuracy: 96.88%\n",
            "============Train on Epoch: 5 \t Loss: 0.0581 \t Accuracy: 98.07%=============\n",
            "=============Test on Epoch: 5 \t Loss: 0.0750 \t Accuracy: 97.77%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.0337 \t Train Accuracy: 98.44%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.1508 \t Train Accuracy: 95.31%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.0979 \t Train Accuracy: 93.75%\n",
            "============Train on Epoch: 6 \t Loss: 0.0507 \t Accuracy: 98.35%=============\n",
            "=============Test on Epoch: 6 \t Loss: 0.0866 \t Accuracy: 97.54%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.0560 \t Train Accuracy: 96.88%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.0275 \t Train Accuracy: 98.44%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.0662 \t Train Accuracy: 96.88%\n",
            "============Train on Epoch: 7 \t Loss: 0.0448 \t Accuracy: 98.58%=============\n",
            "=============Test on Epoch: 7 \t Loss: 0.0956 \t Accuracy: 97.40%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.0176 \t Train Accuracy: 100.00%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.1897 \t Train Accuracy: 93.75%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.0462 \t Train Accuracy: 96.88%\n",
            "============Train on Epoch: 8 \t Loss: 0.0425 \t Accuracy: 98.66%=============\n",
            "=============Test on Epoch: 8 \t Loss: 0.0814 \t Accuracy: 97.68%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.0081 \t Train Accuracy: 100.00%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.0164 \t Train Accuracy: 100.00%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.0257 \t Train Accuracy: 98.44%\n",
            "============Train on Epoch: 9 \t Loss: 0.0380 \t Accuracy: 98.68%=============\n",
            "=============Test on Epoch: 9 \t Loss: 0.0772 \t Accuracy: 97.78%=============\n",
            "Iter: [19200/60000 (32.00%)] \t Train Loss: 0.0626 \t Train Accuracy: 96.88%\n",
            "Iter: [38400/60000 (64.00%)] \t Train Loss: 0.0260 \t Train Accuracy: 100.00%\n",
            "Iter: [57600/60000 (96.00%)] \t Train Loss: 0.0122 \t Train Accuracy: 100.00%\n",
            "============Train on Epoch: 10 \t Loss: 0.0361 \t Accuracy: 98.80%============\n",
            "============Test on Epoch: 10 \t Loss: 0.0786 \t Accuracy: 97.85%=============\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}