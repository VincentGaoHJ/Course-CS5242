{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "practice_2_linearLogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VincentGaoHJ/Course-CS5242/blob/master/practice_2_linearLogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2R1Vbg0FduI"
      },
      "source": [
        "## Practice 2: Pytorch autograd - Linear and Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ua8dIu7F5v_"
      },
      "source": [
        "Pytorch has an autograd feature which is essential for Deep Learning, as it is required for the framework to automatically figure out gradients of loss functions w.r.t all the parameters of any model.\n",
        "\n",
        "In this tutorial, we will learn the PyTorch Autograd feature and use it to solve linear regression and logistic regression problems.\n",
        "\n",
        "NOTE: For the whole notebook perform all your operation and instantiate all the variables on a GPU. (Google collab allows you to make environments with a GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHVBgqT_dhAD"
      },
      "source": [
        "We start by importing pytorch and numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj2x2fk6ViJv"
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m3FfJG5dn3u"
      },
      "source": [
        "We need to set device for the environment, here we have only one gpu. Gpus are indexed starting from 0, as we have only one gpu, we will use that one.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdvKw2M0q_bR",
        "outputId": "006c48e6-f286-4acc-9a92-65269fc8f91d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jan 27 00:36:31 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzVFcsdcVyNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0746f9ba-4052-4741-97e3-d1a120794ac4"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "device"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9Us9JfRfFYj"
      },
      "source": [
        "**Q1. We consider the linear regression case, where we have data x, y and we want to find w,\n",
        "b so that the data follows y = x * w + b approximately.**\n",
        "1. Create a random normal tensor x having dimension (10, 3), where 10 denotes\n",
        "sample size and 3 denotes feature size.\n",
        "2. Create two randn tensors w_true, b_true having size 3 and 1 respectively.\n",
        "Multiply 5 with w and -2 with b .\n",
        "3. Generate tensor y_true by using the equation y_true = x * w_true + b_true\n",
        "\n",
        "Note that this value of w_true and b_true will not be used henceforth, rather it is only being\n",
        "used to generate the data for our regression problem, and to compare at the end our fitted w, b\n",
        "with the true values we have here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44A11qyxrm2v",
        "outputId": "7156c2a7-7d1b-425f-9389-f55105b28676"
      },
      "source": [
        "# Create a random normal tensor x having dimension (10, 3), where 10 denotes sample size and 3 denotes feature size\r\n",
        "x = torch.randn(10, 3)\r\n",
        "print(\"x:\\n\", x)\r\n",
        "\r\n",
        "# Create two randn tensors w_true, b_true having size 3 and 1 respectively. Multiply 5 with w and -2 with b\r\n",
        "w_true = torch.randn(3) * 5\r\n",
        "b_true = torch.randn(1) - 2\r\n",
        "print(\"w_true:\\n\", w_true)\r\n",
        "print(\"b_true:\\n\", b_true)\r\n",
        "\r\n",
        "# Generate tensor y_true by using the equation y_true = x * w_true + b_true\r\n",
        "y_true = torch.matmul(x, w_true) + b_true\r\n",
        "print(\"y_true = x * w_true + b_true:\\n\", y_true)\r\n",
        "\r\n",
        "# The difference between torch.mm, torch.mul, torch.matmul\r\n",
        "# torch.mm(a, b): (3, 4) * (4, 2) -> ​​(3, 2)\r\n",
        "# torch.matmul(a, b): (3, 4) * (5, 4, 2) -> ​​(5, 3, 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            " tensor([[ 0.1258,  2.0514, -0.3690],\n",
            "        [ 0.7158,  1.0307,  0.6046],\n",
            "        [ 0.8109,  0.7432,  0.4827],\n",
            "        [-0.1428, -1.0949, -0.6134],\n",
            "        [-0.8011,  0.1795,  0.0511],\n",
            "        [-2.0741,  0.2468, -1.5186],\n",
            "        [-1.1366, -1.4331,  0.1144],\n",
            "        [-0.8764,  0.5175,  1.3571],\n",
            "        [ 2.0504, -0.5699,  0.9009],\n",
            "        [-0.5387, -0.5388,  0.8024]])\n",
            "w_true:\n",
            " tensor([ 1.0543, -4.3083,  3.8480])\n",
            "b_true:\n",
            " tensor([-3.0357])\n",
            "y_true = x * w_true + b_true:\n",
            " tensor([-13.1610,  -4.3954,  -3.5253,  -0.8292,  -4.4570, -12.1294,   2.3804,\n",
            "         -0.9673,   5.0481,   1.8052])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K3lBJhRfFYo"
      },
      "source": [
        "**Q2. Now define two randn tensor w, b of the same size as w_true and b_true , but with\n",
        "[requires_grad](https://pytorch.org/docs/stable/notes/autograd.html) switched on. Calculate a tensor y = x * w + b , check whether this y\n",
        "has requires_grad switched on.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdN9RpPVx_G7",
        "outputId": "8b148d08-50b3-4775-f92b-b9e54dc4af34"
      },
      "source": [
        "# Define two randn tensor w, b of the same size as w_true and b_true\r\n",
        "w = torch.randn(3, requires_grad=True)\r\n",
        "b = torch.randn(1, requires_grad=True)\r\n",
        "print(\"w:\\n\", w)\r\n",
        "print(\"b:\\n\", b)\r\n",
        "\r\n",
        "# requires_grad switched on. Calculate a tensor y = x * w + b\r\n",
        "y = torch.matmul(x, w) + b\r\n",
        "print(\"y = x * w + b:\\n\", y)\r\n",
        "\r\n",
        "# check whether this y has requires_grad switched on.\r\n",
        "print(\"Check requires_grad: \", y.requires_grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w:\n",
            " tensor([0.1856, 1.8790, 0.1220], requires_grad=True)\n",
            "b:\n",
            " tensor([-1.1403], requires_grad=True)\n",
            "y = x * w + b:\n",
            " tensor([ 2.6926,  1.0029,  0.4654, -3.2989, -0.9454, -1.2467, -4.0301, -0.1650,\n",
            "        -1.7208, -2.1548], grad_fn=<AddBackward0>)\n",
            "Check requires_grad:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QbjpYcIfFYt"
      },
      "source": [
        "**Q3. Define a loss function loss which takes w and b as arguments and returns loss which is\n",
        "squared error = $\\sum(y-y_{true})^2$, where y = x * w + b.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJdNpZvgYaIe"
      },
      "source": [
        "def loss_func(output, target):\n",
        "    loss = torch.sum((output - target)**2)\n",
        "    return loss"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBb-kC1BfFYz"
      },
      "source": [
        "**Q4. Create a notebook cell with the following tasks**\n",
        "1. Call the loss function and check whether the calculated loss has requires_grad\n",
        "switched on\n",
        "2. Call .backward() for the loss. Check gradients of w and b after this.\n",
        "3. At this point repeatedly execute this cell, print values of loss, w, b and also the\n",
        "gradients of w and b . Notice the changes in the gradient value of w, b and no change\n",
        "in loss, y, w, b.\n",
        "4. Now manually set the gradients of w and b to zero, and re-execute this whole cell\n",
        "multiple times and check the gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXreHy6T11YO",
        "outputId": "2cf61b93-f13e-415b-f9ae-eb62c54ff367"
      },
      "source": [
        "# Call the loss function and check whether the calculated loss has requires_grad switched on\r\n",
        "loss = loss_func(y, y_true)\r\n",
        "print(\"loss:\\n\", loss)\r\n",
        "print(\"Check loss requires_grad: \", loss.requires_grad)\r\n",
        "\r\n",
        "def run_backward(reset_grad=False):\r\n",
        "\r\n",
        "    # manually set the gradients of w and b to zero\r\n",
        "    if reset_grad:\r\n",
        "        w.grad.zero_()\r\n",
        "        b.grad.zero_()\r\n",
        "    \r\n",
        "    # Call .backward() for the loss.\r\n",
        "    loss.backward(retain_graph = True)\r\n",
        "\r\n",
        "    # Check w and b\r\n",
        "    print(\"w: \", w)\r\n",
        "    print(\"b: \", b)\r\n",
        "\r\n",
        "    # Check gradients of w and b\r\n",
        "    print(\"Gradients of w: \", w.grad)\r\n",
        "    print(\"Gradients of b: \", b.grad)\r\n",
        "\r\n",
        "run_backward()\r\n",
        "# At this point repeatedly execute this cell, print values of loss, w, b and also the gradients of w and b . \r\n",
        "# Notice the changes in the gradient value of w, b and no change in loss, y, w, b.\r\n",
        "for _ in range(4):\r\n",
        "    print('Excute'.center(60, '='))\r\n",
        "    run_backward()\r\n",
        "\r\n",
        "# Now manually set the gradients of w and b to zero\r\n",
        "# re-execute this whole cell multiple times and check the gradients\r\n",
        "for _ in range(4):\r\n",
        "    print('Excute with zero gradients'.center(60, '='))\r\n",
        "    run_backward(reset_grad=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss:\n",
            " tensor(536.5044, grad_fn=<SumBackward0>)\n",
            "Check loss requires_grad:  True\n",
            "w:  tensor([0.1856, 1.8790, 0.1220], requires_grad=True)\n",
            "b:  tensor([-1.1403], requires_grad=True)\n",
            "Gradients of w:  tensor([-42.2015, 125.3335, -48.8236])\n",
            "Gradients of b:  tensor([41.6599])\n",
            "===========================Excute===========================\n",
            "w:  tensor([0.1856, 1.8790, 0.1220], requires_grad=True)\n",
            "b:  tensor([-1.1403], requires_grad=True)\n",
            "Gradients of w:  tensor([-84.4031, 250.6670, -97.6473])\n",
            "Gradients of b:  tensor([83.3198])\n",
            "===========================Excute===========================\n",
            "w:  tensor([0.1856, 1.8790, 0.1220], requires_grad=True)\n",
            "b:  tensor([-1.1403], requires_grad=True)\n",
            "Gradients of w:  tensor([-126.6046,  376.0005, -146.4709])\n",
            "Gradients of b:  tensor([124.9797])\n",
            "===========================Excute===========================\n",
            "w:  tensor([0.1856, 1.8790, 0.1220], requires_grad=True)\n",
            "b:  tensor([-1.1403], requires_grad=True)\n",
            "Gradients of w:  tensor([-168.8061,  501.3340, -195.2946])\n",
            "Gradients of b:  tensor([166.6396])\n",
            "===========================Excute===========================\n",
            "w:  tensor([0.1856, 1.8790, 0.1220], requires_grad=True)\n",
            "b:  tensor([-1.1403], requires_grad=True)\n",
            "Gradients of w:  tensor([-211.0077,  626.6674, -244.1182])\n",
            "Gradients of b:  tensor([208.2995])\n",
            "=================Excute with zero gradients=================\n",
            "w:  tensor([0.1856, 1.8790, 0.1220], requires_grad=True)\n",
            "b:  tensor([-1.1403], requires_grad=True)\n",
            "Gradients of w:  tensor([-42.2015, 125.3335, -48.8236])\n",
            "Gradients of b:  tensor([41.6599])\n",
            "=================Excute with zero gradients=================\n",
            "w:  tensor([0.1856, 1.8790, 0.1220], requires_grad=True)\n",
            "b:  tensor([-1.1403], requires_grad=True)\n",
            "Gradients of w:  tensor([-42.2015, 125.3335, -48.8236])\n",
            "Gradients of b:  tensor([41.6599])\n",
            "=================Excute with zero gradients=================\n",
            "w:  tensor([0.1856, 1.8790, 0.1220], requires_grad=True)\n",
            "b:  tensor([-1.1403], requires_grad=True)\n",
            "Gradients of w:  tensor([-42.2015, 125.3335, -48.8236])\n",
            "Gradients of b:  tensor([41.6599])\n",
            "=================Excute with zero gradients=================\n",
            "w:  tensor([0.1856, 1.8790, 0.1220], requires_grad=True)\n",
            "b:  tensor([-1.1403], requires_grad=True)\n",
            "Gradients of w:  tensor([-42.2015, 125.3335, -48.8236])\n",
            "Gradients of b:  tensor([41.6599])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiSttOvVftZ-"
      },
      "source": [
        "**Q5. Linear Regression with shallow networks. Tie all the operations together in a different cell with the following steps**\n",
        "1. Re-initialize w and b with random numbers.\n",
        "2. Calculate loss involving unknown parameters w, b\n",
        "3. Calculate gradient by calling backward on loss\n",
        "4. Use gradients to update values of parameters (gradient descent update: Keep learning rate 0.01)\n",
        "5. Set gradients to zero\n",
        "6. Go to step 2 until convergence (value of the loss is less than tolerance, set the tolerance to\n",
        "1e-5 )\n",
        "7. Check whether the value of w and b is close to the true values, i.e. w_true and b_true"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btvMMBGBby5g"
      },
      "source": [
        "class LinearRegression():\n",
        "    def __init__(self):\n",
        "        self.w = torch.randn(3, requires_grad=True)\n",
        "        self.b = torch.randn(1, requires_grad=True)\n",
        "        self.loss = float(\"inf\")\n",
        "        self.tolerance = 1e-5\n",
        "        self.iter_num = 0\n",
        "\n",
        "    def train(self, x_train, y_true):\n",
        "        print('Start Training'.center(50,'='))\n",
        "        while self.loss >= self.tolerance:\n",
        "            self.iteration(x_train, y_true)\n",
        "            self.iter_num += 1\n",
        "            if self.iter_num % 10 == 0:\n",
        "                print(f\"iter {self.iter_num} loss: {self.loss}\")\n",
        "            if self.iter_num >= 1000: break\n",
        "        print('Training Complete'.center(50,'='))\n",
        "\n",
        "    def iteration(self, x_train, y_true):\n",
        "        y_pred = self.predict(x_train)\n",
        "        self.loss = loss_func(y_pred, y_true) # Calculate loss\n",
        "        self.loss.backward()\n",
        "        self.update_parameters()\n",
        "\n",
        "    def predict(self, input):\n",
        "        y_pred = torch.matmul(input, self.w) + self.b\n",
        "        return y_pred\n",
        "\n",
        "    def update_parameters(self, learning_rate=0.01):\n",
        "        # If we don't use torch.no_grad then weight update step will be added to \n",
        "        # the computation graph of the Neural Network which is not desired.\n",
        "        with torch.no_grad():\n",
        "            # Update values of parameters\n",
        "            self.w -= self.w.grad * learning_rate\n",
        "            self.b -= self.b.grad * learning_rate\n",
        "            # Set gradients to zero\n",
        "            self.w.grad.zero_()\n",
        "            self.b.grad.zero_()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL3pVluoxs_0",
        "outputId": "d0522b10-3ce3-4109-e123-097067edae71"
      },
      "source": [
        "# Data Generation\r\n",
        "x = torch.randn(10, 3) # Create a random normal tensor x\r\n",
        "w_true = torch.randn(3) * 5 # Create two randn tensors w_true, b_true\r\n",
        "b_true = torch.randn(1) - 2\r\n",
        "y_true = torch.matmul(x, w_true) + b_true # Generate tensor y_true\r\n",
        "\r\n",
        "# Model training\r\n",
        "lr_model = LinearRegression()\r\n",
        "lr_model.train(x_train=x, y_true=y_true)\r\n",
        "y_pred = lr_model.predict(x)\r\n",
        "loss = loss_func(y_pred, y_true)\r\n",
        "print(f'The Final Loss is: {loss}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================Start Training==================\n",
            "iter 10 loss: 65.78162384033203\n",
            "iter 20 loss: 11.58525562286377\n",
            "iter 30 loss: 2.0746517181396484\n",
            "iter 40 loss: 0.3730771541595459\n",
            "iter 50 loss: 0.06722046434879303\n",
            "iter 60 loss: 0.012130334042012691\n",
            "iter 70 loss: 0.002191745676100254\n",
            "iter 80 loss: 0.0003965335781686008\n",
            "iter 90 loss: 7.179342355811968e-05\n",
            "iter 100 loss: 1.3017619494348764e-05\n",
            "================Training Complete=================\n",
            "The Final Loss is: 7.807415386196226e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yUqq3yUgKhc"
      },
      "source": [
        "**Q6. Now we will change the problem to a Logistic Regression Problem**\n",
        "1. Change y_true to be 1 if y_true > 0 else 0 , make it a float tensor and make sure it is\n",
        "on the device.\n",
        "2. Define a [new loss function to include a sigmoid transformation](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) of prediction y , to make it a\n",
        "probability. [Change the loss to cross-entropy loss for binary classification](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) with probability y.\n",
        "3. Repeat the steps of Q5 for this problem and check the final values of w and b, make\n",
        "stopping criteria to be loss value <= 0.05 . Keep learning rate the same as before.\n",
        "4. You might notice that you do not recover the w_true and b_true value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o86A2IsLrjPL"
      },
      "source": [
        "class LogisticRegression():\r\n",
        "    def __init__(self, tolerance=1e-5, max_iter=1000, log_round=10):\r\n",
        "        self.w = torch.randn(3, requires_grad=True)\r\n",
        "        self.b = torch.randn(1, requires_grad=True)\r\n",
        "        self.loss = float(\"inf\")\r\n",
        "        self.tolerance = tolerance\r\n",
        "        self.iter_num = 0\r\n",
        "        self.loss_func = torch.nn.BCEWithLogitsLoss()\r\n",
        "        self.max_iter = max_iter\r\n",
        "        self.log_round = log_round\r\n",
        "\r\n",
        "    def train(self, x_train, y_true):\r\n",
        "        print('Start Training'.center(50,'='))\r\n",
        "        while self.loss >= self.tolerance:\r\n",
        "            self.iteration(x_train, y_true)\r\n",
        "            self.iter_num += 1\r\n",
        "            if self.iter_num % self.log_round == 0:\r\n",
        "                print(f\"iter {self.iter_num} loss: {self.loss}\")\r\n",
        "            if self.iter_num >= self.max_iter: break\r\n",
        "        print('Training Complete'.center(50,'='))\r\n",
        "\r\n",
        "    def iteration(self, x_train, y_true):\r\n",
        "        y_pred = self.predict(x_train)\r\n",
        "        self.loss = self.loss_func(y_pred, y_true) # Calculate loss\r\n",
        "        self.loss.backward()\r\n",
        "        self.update_parameters()\r\n",
        "\r\n",
        "    def predict(self, input):\r\n",
        "        y_pred = torch.matmul(input, self.w) + self.b\r\n",
        "        return y_pred\r\n",
        "\r\n",
        "    def update_parameters(self, learning_rate=0.01):\r\n",
        "        # If we don't use torch.no_grad then weight update step will be added to \r\n",
        "        # the computation graph of the Neural Network which is not desired.\r\n",
        "        with torch.no_grad():\r\n",
        "            # Update values of parameters\r\n",
        "            self.w -= self.w.grad * learning_rate\r\n",
        "            self.b -= self.b.grad * learning_rate\r\n",
        "            # Set gradients to zero\r\n",
        "            self.w.grad.zero_()\r\n",
        "            self.b.grad.zero_()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IB15Nwebk8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "763309ba-096b-4ac2-bd42-0a68ec864236"
      },
      "source": [
        "# Data Generation\n",
        "x = torch.randn(10, 3) # Create a random normal tensor x\n",
        "w_true = torch.randn(3) * 5 # Create two randn tensors w_true, b_true\n",
        "b_true = torch.randn(1) - 2\n",
        "y_true = torch.matmul(x, w_true) + b_true # Generate tensor y_true\n",
        "y_true_binary = y_true > 0 # generate the boolean mask\n",
        "y_true_binary = y_true_binary.float()\n",
        "\n",
        "# Model training\n",
        "log_model = LogisticRegression(tolerance=0.05, max_iter=20000, log_round=500)\n",
        "log_model.train(x_train=x, y_true=y_true_binary)\n",
        "y_pred = log_model.predict(x)\n",
        "loss = log_model.loss_func(y_pred, y_true_binary)\n",
        "print(f'y_pred: {y_pred}')\n",
        "print(f'y_true_binary: {y_true_binary}')\n",
        "print(f'The Final Loss: {loss}')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================Start Training==================\n",
            "iter 500 loss: 0.3847920298576355\n",
            "iter 1000 loss: 0.22251686453819275\n",
            "iter 1500 loss: 0.16936607658863068\n",
            "iter 2000 loss: 0.14107182621955872\n",
            "iter 2500 loss: 0.12265173345804214\n",
            "iter 3000 loss: 0.1093241423368454\n",
            "iter 3500 loss: 0.09904482215642929\n",
            "iter 4000 loss: 0.09077397733926773\n",
            "iter 4500 loss: 0.0839180201292038\n",
            "iter 5000 loss: 0.078109011054039\n",
            "iter 5500 loss: 0.0731038898229599\n",
            "iter 6000 loss: 0.06873401999473572\n",
            "iter 6500 loss: 0.06487777084112167\n",
            "iter 7000 loss: 0.06144460290670395\n",
            "iter 7500 loss: 0.05836523324251175\n",
            "iter 8000 loss: 0.05558543652296066\n",
            "iter 8500 loss: 0.05306225270032883\n",
            "iter 9000 loss: 0.0507606640458107\n",
            "================Training Complete=================\n",
            "y_pred: tensor([ 4.0638,  4.0347, -4.3769, -4.7302, -3.5184,  1.5832, -7.7064,  3.4150,\n",
            "        -1.6345, -4.0581], grad_fn=<AddBackward0>)\n",
            "y_true_binary: tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 0.])\n",
            "The Final Loss is: 0.04999345541000366\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}