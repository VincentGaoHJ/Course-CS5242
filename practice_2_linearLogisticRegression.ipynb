{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "practice_2_linearLogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VincentGaoHJ/Course-CS5242/blob/master/practice_2_linearLogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2R1Vbg0FduI"
      },
      "source": [
        "## Practice 2: Pytorch autograd - Linear and Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ua8dIu7F5v_"
      },
      "source": [
        "Pytorch has an autograd feature which is essential for Deep Learning, as it is required for the framework to automatically figure out gradients of loss functions w.r.t all the parameters of any model.\n",
        "\n",
        "In this tutorial, we will learn the PyTorch Autograd feature and use it to solve linear regression and logistic regression problems.\n",
        "\n",
        "NOTE: For the whole notebook perform all your operation and instantiate all the variables on a GPU. (Google collab allows you to make environments with a GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHVBgqT_dhAD"
      },
      "source": [
        "We start by importing pytorch and numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj2x2fk6ViJv"
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m3FfJG5dn3u"
      },
      "source": [
        "We need to set device for the environment, here we have only one gpu. Gpus are indexed starting from 0, as we have only one gpu, we will use that one.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzVFcsdcVyNU"
      },
      "source": [
        "device = torch.device('cuda:0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9Us9JfRfFYj"
      },
      "source": [
        "**Q1. We consider the linear regression case, where we have data x, y and we want to find w,\n",
        "b so that the data follows y = x * w + b approximately.**\n",
        "1. Create a random normal tensor x having dimension (10, 3), where 10 denotes\n",
        "sample size and 3 denotes feature size.\n",
        "2. Create two randn tensors w_true, b_true having size 3 and 1 respectively.\n",
        "Multiply 5 with w and -2 with b .\n",
        "3. Generate tensor y_true by using the equation y_true = x * w_true + b_true\n",
        "\n",
        "Note that this value of w_true and b_true will not be used henceforth, rather it is only being\n",
        "used to generate the data for our regression problem, and to compare at the end our fitted w, b\n",
        "with the true values we have here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS5fd1GhWHoL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6bef98c5-3219-4673-9e82-e04cb4fe5b15"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.2508,  4.0353, -6.7226], device='cuda:0') tensor([2.0143], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K3lBJhRfFYo"
      },
      "source": [
        "**Q2. Now define two randn tensor w, b of the same size as w_true and b_true , but with\n",
        "requires_grad switched on. Calculate a tensor y = x * w + b , check whether this y\n",
        "has requires_grad switched on.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5kC13n1WdOW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9cf48977-dff9-4514-c3b2-4d1aaa9494f6"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True <AddBackward0 object at 0x7f4d0967a748>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QbjpYcIfFYt"
      },
      "source": [
        "**Q3. Define a loss function loss which takes w and b as arguments and returns loss which is\n",
        "squared error = $\\sum(y-y_{true})^2$, where y = x * w + b.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJdNpZvgYaIe"
      },
      "source": [
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBb-kC1BfFYz"
      },
      "source": [
        "**Q4. Create a notebook cell with the following tasks**\n",
        "1. Call the loss function and check whether the calculated loss has requires_grad\n",
        "switched on\n",
        "2. Call .backward() for the loss. Check gradients of w and b after this.\n",
        "3. At this point repeatedly execute this cell, print values of loss, w, b and also the\n",
        "gradients of w and b . Notice the changes in the gradient value of w, b and no change\n",
        "in loss, y, w, b.\n",
        "4. Now manually set the gradients of w and b to zero, and re-execute this whole cell\n",
        "multiple times and check the gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZf2IsuQY6AF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4811acf6-41e2-4981-f8a7-1896f4245a57"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(772.6249, device='cuda:0', grad_fn=<SumBackward0>) None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvoUv_YuZR_a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10f3dc92-f767-44dc-ae8c-b1dd28b8d46e"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  -0.7408, -149.3755,   92.2842], device='cuda:0') tensor([-63.4368], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKj3dHxCYQ_q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "186a455d-5288-4032-ccaf-a6333d73c1b8"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(772.6249, device='cuda:0', grad_fn=<SumBackward0>) tensor([  -2.9634, -597.5021,  369.1370], device='cuda:0') tensor([-253.7474], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ03OZxfaN7H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3113426-97a1-49de-913c-df011e33da06"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 0., 0.], device='cuda:0') tensor([0.], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiSttOvVftZ-"
      },
      "source": [
        "**Q5. Linear Regression with shallow networks. Tie all the operations together in a different cell with the following steps**\n",
        "1. Re-initialize w and b with random numbers.\n",
        "2. Calculate loss involving unknown parameters w, b\n",
        "3. Calculate gradient by calling backward on loss\n",
        "4. Use gradients to update values of parameters (gradient descent update: Keep learning rate 0.01)\n",
        "5. Set gradients to zero\n",
        "6. Go to step 2 until convergence (value of the loss is less than tolerance, set the tolerance to\n",
        "1e-5 )\n",
        "7. Check whether the value of w and b is close to the true values, i.e. w_true and b_true"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btvMMBGBby5g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "29a6fe7d-aaf5-41a4-a89a-5b5d42ea3493"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Took 94 iterations\n",
            "tensor([-0.2499,  4.0354, -6.7217], device='cuda:0', requires_grad=True) tensor([2.0149], device='cuda:0', requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yUqq3yUgKhc"
      },
      "source": [
        "**Q6. Now we will change the problem to a Logistic Regression Problem**\n",
        "1. Change y_true to be 1 if y_true > 0 else 0 , make it a float tensor and make sure it is\n",
        "on the device.\n",
        "2. Define a new loss function to include a sigmoid transformation of prediction y , to make it a\n",
        "probability. Change the loss to cross-entropy loss for binary classification with probability y.\n",
        "3. Repeat the steps of Q5 for this problem and check the final values of w and b, make\n",
        "stopping criteria to be loss value <= 0.05 . Keep learning rate the same as before.\n",
        "4. You might notice that you do not recover the w_true and b_true value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IB15Nwebk8i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8da5f851-a212-485f-a581-50419e434119"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 0., 1., 1., 0., 0., 1., 1., 1., 1.], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKeJtyeclfFr"
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfRnXxHIlzn5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "73f8c9a8-38cf-4bdb-ee02-14d67180d6b8"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Took 13383 iterations\n",
            "tensor([ -6.7814,   0.7458, -10.0162], device='cuda:0', requires_grad=True) tensor([0.2812], device='cuda:0', requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}